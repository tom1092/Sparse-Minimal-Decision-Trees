{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from TreeStructures import TreeNode, ClassificationTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(data, labels):\n",
    "    positives = data[labels==1]\n",
    "    negatives = data[labels==0]\n",
    "    \n",
    "    m_p = np.mean(positives)\n",
    "    m_n = np.mean(negatives)\n",
    "    #print(np.mean(positives))\n",
    "    #print(np.mean(negatives))\n",
    "    #print(np.std(positives))\n",
    "    #print(np.std(negatives))\n",
    "    #print(\"fine\")\n",
    "    #print(np.exp(np.std(positives)*np.std(negatives)))\n",
    "    #return np.abs((np.mean(positives)-np.mean(negatives))**2)+np.exp(-(np.std(positives)*np.std(negatives)))\n",
    "    value = np.abs(m_p - m_n)**2/(np.std(positives)*np.std(negatives))\n",
    "    if not np.isnan(value):\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "    #return np.abs(m_p - m_n)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split_purity(X, y, data_idxs):\n",
    "    min_impur = np.inf\n",
    "    best_f = None\n",
    "    best_th = None\n",
    "    alfa = 1\n",
    "    #metrics = [metric(X[data_idxs, i], y[data_idxs]) for i in range(len(X[0,:]))]\n",
    "    #ranks = np.argsort(metrics)\n",
    "    #ranks = ranks[::-1]\n",
    "    #feature = ranks[0]\n",
    "    for feature in range(len(X[0])):\n",
    "        indices = np.argsort(X[data_idxs, feature])\n",
    "        subset_X = X[data_idxs[indices], feature]\n",
    "        subset_y = y[data_idxs[indices]]\n",
    "        #for i in range(len(subset_X)-1):\n",
    "            #th = (subset_X[i]+subset_X[i+1])/2\n",
    "            #points_left_indexes = [i for i in range(len(subset_X)) if subset_X[i] <= th]\n",
    "            #points_right_indexes = [i for i in range(len(subset_X)) if subset_X[i] > th]\n",
    "\n",
    "            #n_positive_left = np.count_nonzero(subset_y[points_left_indexes])\n",
    "            #n_negative_left = len(points_left_indexes) - np.count_nonzero(subset_y[points_left_indexes])\n",
    "\n",
    "            #n_positive_right = np.count_nonzero(subset_y[points_right_indexes])\n",
    "            #n_negative_right = len(points_right_indexes) - np.count_nonzero(subset_y[points_right_indexes])\n",
    "        i = 0\n",
    "        n_left, n_right = 0, len(indices)\n",
    "        n_positive_left = 0\n",
    "        n_negative_left = 0\n",
    "        n_positive_right = np.count_nonzero(subset_y)\n",
    "        n_negative_right = n_right - n_positive_right\n",
    "\n",
    "        while (i < len(subset_X) - 1):\n",
    "            th = (subset_X[i]+subset_X[i+1])/2\n",
    "            if subset_y[i] == 0:\n",
    "                n_negative_left += 1\n",
    "                n_negative_right -= 1\n",
    "            else:\n",
    "                n_positive_left += 1\n",
    "                n_positive_right -= 1\n",
    "\n",
    "\n",
    "            k = 1\n",
    "            while k+i < len(subset_X) and subset_X[k+i] == subset_X[i]:\n",
    "                if subset_y[k+i] == 0:\n",
    "                    n_negative_left += 1\n",
    "                    n_negative_right -= 1\n",
    "                else:\n",
    "                    n_positive_left += 1\n",
    "                    n_positive_right -= 1\n",
    "                k+=1\n",
    "\n",
    "            i+=k\n",
    "            n_left += k\n",
    "            n_right += -k\n",
    "\n",
    "            impurity_left = 1-np.abs(n_positive_left - n_negative_left)/len(subset_X)\n",
    "            impurity_right = 1-np.abs(n_positive_right - n_negative_right)/len(subset_X)\n",
    "\n",
    "            acc_mean = 1 - (np.abs(n_positive_left-n_negative_left)+np.abs(n_positive_right-n_negative_right))/len(subset_X)\n",
    "            impurity = alfa*min(impurity_left, impurity_right) + (1-alfa)*acc_mean\n",
    "            if impurity < min_impur:\n",
    "                min_impur = impurity\n",
    "                best_f = feature\n",
    "                best_th = th\n",
    "                \n",
    "        \n",
    "    return best_f, best_th, min_impur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAO:\n",
    "    def __init__(self, tree, oblique = False, train_on_all_features = True):\n",
    "        self.classification_tree = tree\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.train_on_all_features = train_on_all_features\n",
    "\n",
    "    def evolve(self, X, y, n_iter = 7, min_size_prune = 1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(n_iter):\n",
    "            #print(\"TAO iter \", i, \" di \", n_iter)\n",
    "            for depth in reversed(range(self.classification_tree.depth + 1)):\n",
    "                #print(\"Ottimizzo depth\", depth, \"....\")\n",
    "                T = self.classification_tree\n",
    "                nodes = ClassificationTree.get_nodes_at_depth(depth, T)\n",
    "                #print ([node.id for node in nodes])\n",
    "\n",
    "                for node in nodes:\n",
    "                    self.optimize_nodes(node)\n",
    "                \n",
    "                #Rimetto apposto i punti associati ad ogni nodo\n",
    "                self.classification_tree.build_idxs_of_subtree(X, range(len(X)), T.tree[0], oblique = T.oblique)\n",
    "        #ClassificationTree.restore_tree(self.classification_tree)\n",
    "        self.prune()\n",
    "        \n",
    "    \n",
    "    def optimize_nodes(self, node):\n",
    "            #print (\"node id = \", node.id, \"depth = \", node.depth)\n",
    "            #print(\"ottimizzo nodo: \", node.id)\n",
    "            T = self.classification_tree\n",
    "            X = self.X\n",
    "            y = self.y\n",
    "            if node.data_idxs:\n",
    "                if node.is_leaf :\n",
    "                    #Se il nodo è una foglia ottimizzo semplicemente settando la predizione\n",
    "                    #alla classe più comune tra le instanze che arrivano a quella foglia\n",
    "                    #print(max(node.data_idxs))\n",
    "                    best_class = np.bincount(y[node.data_idxs]).argmax()\n",
    "                    node.value = best_class\n",
    "                    care_points_idxs = node.data_idxs\n",
    "                    #print(\"Node value: \", node.value)\n",
    "                    #print(\"Best class: \", best_class)\n",
    "                else:\n",
    "                    #In questo caso devo risolvere un problema di minimizzazione\n",
    "                    #I punti da tenere in considerazione sono solo quelli tali che la\n",
    "                    #predizione dei due figli del nodo corrente è diversa, e in almeno\n",
    "                    #uno il punto viene classificato correttamente\n",
    "                    care_points_idxs = []\n",
    "                    correct_classification_tuples = {}\n",
    "                    for data_idx in node.data_idxs:\n",
    "                        left_label_predicted = T.predict_label(X[data_idx].reshape((1, -1)), node.left_node, T.oblique)\n",
    "                        right_label_predicted = T.predict_label(X[data_idx].reshape((1, -1)), node.right_node, T.oblique)\n",
    "                        correct_label = y[data_idx]\n",
    "                        if (left_label_predicted != right_label_predicted and right_label_predicted==correct_label):\n",
    "                            correct_classification_tuples[data_idx] = 1\n",
    "                            care_points_idxs.append(data_idx)\n",
    "                        elif (left_label_predicted != right_label_predicted and left_label_predicted==correct_label):\n",
    "                            care_points_idxs.append(data_idx)\n",
    "                            correct_classification_tuples[data_idx] = 0\n",
    "                    self.TAO_best_split(node.id, X, y, care_points_idxs, correct_classification_tuples, T)\n",
    "                    #print(\"care_points: \", care_points_idxs)\n",
    "\n",
    "                #print (\"indici nodo prima\", node.id, \"--->\", node.data_idxs)\n",
    "                #prec = node.data_idxs\n",
    "                #print (set(prec) == set(node.data_idxs))\n",
    "\n",
    "\n",
    "    def TAO_best_split(self, node_id, X, y, care_points_idxs, correct_classification_tuples, T):\n",
    "\n",
    "        #Itero sulle componenti\n",
    "        #print (\"Ottimizzo nodo:\", node_id)\n",
    "        if len(care_points_idxs) > 1:\n",
    "            #print(len(care_points_idxs))\n",
    "            if T.oblique:\n",
    "                best_t, best_weights = self.TAO_best_SVM_split(node_id, X, y, care_points_idxs, correct_classification_tuples, T)\n",
    "                T.tree[node_id].intercept = best_t\n",
    "                T.tree[node_id].weights = best_weights\n",
    "            else:\n",
    "                best_t, best_feature = self.TAO_best_parallel_split(node_id, X, y, care_points_idxs, correct_classification_tuples, T)\n",
    "                #print(\"finale: \", error_best)\n",
    "                T.tree[node_id].threshold = best_t\n",
    "                T.tree[node_id].feature = best_feature\n",
    "\n",
    "\n",
    "    #Qui faccio uno split parallelo agli assi\n",
    "    def TAO_best_parallel_split(self, node_id, X, y, care_points_idxs, correct_classification_tuples, T):\n",
    "        #print (\"node id:\", node_id)\n",
    "        error_best = np.inf\n",
    "        best_t = T.tree[node_id].threshold\n",
    "        best_feature = T.tree[node_id].feature\n",
    "        if self.train_on_all_features:\n",
    "            features = range(len(X[0]))\n",
    "        else:\n",
    "            features = ClassificationTree.get_features(T)\n",
    "\n",
    "        for j in features:\n",
    "            #print(j)\n",
    "            #Prendo tutte le j-esime componenti del dataset e le ordino\n",
    "            #in modo crescente\n",
    "            vals = {}\n",
    "            for point_idx in care_points_idxs:\n",
    "                vals[point_idx] = X[point_idx, j]\n",
    "\n",
    "            values = sorted(vals.items(), key=lambda x: x[1])\n",
    "            sorted_indexes = [tuple[0] for tuple in values]\n",
    "            #plt.scatter(X[sorted_indexes, j], range(len(values)), s=0.4, c=list(correct_classification_tuples.values()))\n",
    "            #plt.show()\n",
    "            T.tree[node_id].feature = j\n",
    "            thresh = 0.5*(X[sorted_indexes[0], j]+X[sorted_indexes[1], j])\n",
    "            actual_loss, start = self.zero_one_loss(node_id, X, y, sorted_indexes, correct_classification_tuples, thresh, sorted_indexes)\n",
    "\n",
    "            #if j==2:\n",
    "                #base = actual_loss\n",
    "            #actual_loss = self.binary_loss(node_id, X, y, sorted_indexes[i], correct_classification_tuples[sorted_indexes[i]], actual_loss, thresh)\n",
    "            #print (\"loss: \", actual_loss, \"n punti: \", len(care_points_idxs))\n",
    "            #print(\"vecchia: \", self.vecchia_loss(node_id, X, y, care_points_idxs, correct_classification_tuples, thresh))\n",
    "            #Ciclo su ogni valore di quella componente e provo tutti gli split\n",
    "            #possibili\n",
    "            if actual_loss < error_best:\n",
    "                error_best = actual_loss\n",
    "                best_t = thresh\n",
    "                best_feature = j\n",
    "            i = start\n",
    "            while i < len(sorted_indexes)-1:\n",
    "\n",
    "\n",
    "                thresh = 0.5*(X[sorted_indexes[i], j]+X[sorted_indexes[i+1], j])\n",
    "\n",
    "                #sum, k = self.binary_loss(X, j, sorted_indexes, i, correct_classification_tuples)\n",
    "                #actual_loss += sum\n",
    "\n",
    "\n",
    "                #Qualche print per debug\n",
    "                '''\n",
    "                if j==2 and node_id==3:\n",
    "                    ones = [correct_classification_tuples[k] for k in sorted_indexes]\n",
    "                    print(\"base: \", base, \"  n punti:\", len(sorted_indexes), \"    loss:\", actual_loss, \"     thresh:\", thresh, \"     x:\", X[sorted_indexes[i], j], \"    prediction: \", correct_classification_tuples[sorted_indexes[i]])\n",
    "                    print(X[sorted_indexes, j])\n",
    "                    print()\n",
    "                '''\n",
    "                #actual_loss = self.zero_one_loss(node_id, X, y, care_points_idxs, correct_classification_tuples, thresh)\n",
    "\n",
    "                s, k = self.binary_loss(X, j, sorted_indexes, i, correct_classification_tuples)\n",
    "                #print (\"dopo binary\")\n",
    "                actual_loss += s\n",
    "                if actual_loss < error_best:\n",
    "                    error_best = actual_loss\n",
    "                    best_t = thresh\n",
    "                    best_feature = j\n",
    "                i+=k\n",
    "\n",
    "            '''\n",
    "            #print(error_best)\n",
    "            #errors = [self.binary_loss(node_id, X, y, care_points_idxs, correct_classification_tuples, threshes[i]) for i in range(len(values)-1)]\n",
    "            #min_index = np.argmin(errors)\n",
    "        '''\n",
    "        #Ancora print per debug\n",
    "        #if node_id==1:\n",
    "            #print(\"finale: \", error_best)\n",
    "            #print (\"ones:\", ones, \" len:  \", len(ones), \"   care_p_len:\", len(care_points_idxs))\n",
    "            #print(\"best_feature:\", best_feature, \"   best_thresh:\", best_t)\n",
    "\n",
    "\n",
    "\n",
    "        return best_t, best_feature\n",
    "\n",
    "\n",
    "    #Alla fine tolgo dead branches e pure subtrees\n",
    "    def prune(self, min_size=1):\n",
    "        #Prima controllo se ci sono sottoalberi puri.\n",
    "        #Visito l'albero e verifico se esistono nodi branch ai quali arrivano punti associati a solo una label\n",
    "        #Poi vedo se il nodo attuale è morto\n",
    "        T = self.classification_tree\n",
    "        stack = [T.tree[0]]\n",
    "        while (stack):\n",
    "            actual = stack.pop()\n",
    "            if len(actual.data_idxs) > 0:\n",
    "                #Se il nodo è puro\n",
    "                if not actual.is_leaf and all(i == self.y[actual.data_idxs[0]] for i in self.y[actual.data_idxs]):\n",
    "                    #Devo far diventare una foglia questo nodo con valore pari alla label\n",
    "                    actual.is_leaf = True\n",
    "                    actual.left_node = None\n",
    "                    actual.right_node = None\n",
    "                    actual.left_node_id = -1\n",
    "                    actual.right_node_id = -1\n",
    "                    actual.value = self.y[actual.data_idxs[0]]\n",
    "\n",
    "                #Se il nodo ha un figlio morto devo sostituire il padre con l'altro figlio\n",
    "                elif not actual.is_leaf and len(actual.left_node.data_idxs) < min_size:\n",
    "                    stack.append(actual.right_node)\n",
    "                    ClassificationTree.replace_node(actual, actual.right_node, T)\n",
    "                elif not actual.is_leaf and len(actual.right_node.data_idxs) < min_size:\n",
    "                    stack.append(actual.left_node)\n",
    "                    ClassificationTree.replace_node(actual, actual.left_node, T)\n",
    "                elif not actual.is_leaf:\n",
    "                    stack.append(actual.right_node)\n",
    "                    stack.append(actual.left_node)\n",
    "            ClassificationTree.restore_tree(T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Definico la loss base del nodo.\n",
    "    #Significa che parto considerando come thresh la metà del minimo valore su quella componente.\n",
    "    #Questo mi consente di evitare un O(n) e quindi\n",
    "    #di ciclare su ogni punto del dataset per calcolare di nuovo la loss cambiando il\n",
    "    #thresh. Ad ogni cambio, essendo i dati ordinati per componente la loss cambia\n",
    "    #al più di una unità +/- 1. OCCORRE FARE ATTENZIONE A PUNTI UGUALI! VEDI BINARY LOSS\n",
    "    def zero_one_loss(self, node_id, X, y, care_points_idxs, correct_classification_tuples, thresh, sorted_indexes):\n",
    "        loss = 0\n",
    "        T = self.classification_tree.tree\n",
    "        node = T[node_id]\n",
    "        #Per ogni punto del nodo verifico dove questo lo inoltrerebbe\n",
    "        #se il punto viene inoltrato su un figlio che porta a misclassificazione\n",
    "        #è errore\n",
    "        targets = [correct_classification_tuples[x] for x in care_points_idxs]\n",
    "        data = X[care_points_idxs]\n",
    "        predictions = np.array([0 if sample[node.feature] <= thresh else 1 for sample in data[:,]])\n",
    "        n_misclassified = np.count_nonzero(targets-predictions)\n",
    "        k=0\n",
    "        i=0\n",
    "        #print(\"prima del while\")\n",
    "        while k+i< len(sorted_indexes) and X[sorted_indexes[k+i], node.feature] == X[sorted_indexes[i], node.feature]:\n",
    "            k+=1\n",
    "        return n_misclassified, k\n",
    "        #return loss\n",
    "\n",
    "\n",
    "    #Definisco la loss in eq (4) per il problema al singolo nodo\n",
    "    def binary_loss(self, X, feature, sorted_indexes, i, targets):\n",
    "\n",
    "        #Dato un punto del nodo verifico dove questo lo inoltrerebbe\n",
    "        #se il punto viene inoltrato su un figlio che porta a misclassificazione\n",
    "        #è errore\n",
    "        #print(\"loss\")\n",
    "        #sum = 0\n",
    "        #k = 1\n",
    "        #if targets[sorted_indexes[i]] == 0:\n",
    "            #sum -= 1\n",
    "        #else:\n",
    "            #sum +=1\n",
    "\n",
    "        #Vedo se ci sono punti uguali che potrebbero dar fastidio con la loss.\n",
    "        #Se mi fermassi a vedere solo il primo potrei ottenere che lo split migliore\n",
    "        #sia uno tale che dopo ci sono altri punti uguali che però sono della classe sbagliata\n",
    "        #se dopo provassi a splittare su quei punti la loss ovviamente si alzerebbe ma\n",
    "        #nel frattempo, essendo uguali, mi sarei salvato come miglior split proprio il primo.\n",
    "        #Occorre decidere se conviene eseguire lo split guardando insieme tutti i punti uguali\n",
    "        #costruendo una loss per maggioranza\n",
    "        #devo quindi pesare la loss su quanti punti sono classificati bene tra quelli uguali\n",
    "        #dunque devo guardare i consecutivi\n",
    "        sum = 0\n",
    "        k = 0\n",
    "        while k+i < len(sorted_indexes) and X[sorted_indexes[k+i], feature] == X[sorted_indexes[i], feature]:\n",
    "            if targets[sorted_indexes[k+i]] == 0:\n",
    "                sum -= 1\n",
    "            else:\n",
    "                sum +=1\n",
    "            k+=1\n",
    "        #print (k)\n",
    "        return sum, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_tree(X, y, n_splits, min_points_leaf = 4, min_impurity = 1e-1):\n",
    "    key = 0\n",
    "    depth = 0\n",
    "    \n",
    "    node = TreeNode(key, depth)\n",
    "    node.data_idxs = np.array(range(len(X)))\n",
    "    node.depth = 0\n",
    "    positive = np.count_nonzero(y)\n",
    "    negative = len(node.data_idxs) - positive\n",
    "    impurity = 1 -  np.abs(positive - negative)/len(node.data_idxs)\n",
    "    node.impurity = impurity\n",
    "    \n",
    "\n",
    "    stack = [node]\n",
    "    i = 0\n",
    "    while(stack):\n",
    "        stack.sort(key=lambda x: x.impurity)\n",
    "        \n",
    "        n = stack.pop()\n",
    "        \n",
    "        #SET node attributes to the best\n",
    "        n.threshold = None\n",
    "        n.feature, n.threshold , p = get_best_split_purity(X, y, n.data_idxs)\n",
    "        \n",
    "        if n.threshold != None and i<n_splits:\n",
    "            #print(\"Node: \", n.id, \"Feature\", n.feature, \"Th: \", n.threshold, \"Impurity: \", p)\n",
    "            \n",
    "            #Get indexes of left and right subset\n",
    "            indexes_left = np.array([i for i in n.data_idxs if X[i, n.feature] <= n.threshold])\n",
    "            indexes_right = np.array(list(set(n.data_idxs) - set(indexes_left)))\n",
    "            \n",
    "            #print(len(indexes_right))\n",
    "            print(\"left:\", len(indexes_left))\n",
    "            print(\"right:\", len(indexes_right))\n",
    "            \n",
    "            if (len(indexes_left) >= min_points_leaf and len(indexes_right) >= min_points_leaf):\n",
    "                #Get purity of the two subsets\n",
    "                #print(n.feature, n.threshold)\n",
    "                #print(\"si\")\n",
    "                positive_r = np.count_nonzero(y[indexes_right])\n",
    "                negative_r = len(indexes_right) - positive_r\n",
    "                impurity_right = 1 - np.abs(positive_r - negative_r)/len(indexes_right)\n",
    "\n",
    "                positive_l = np.count_nonzero(y[indexes_left])\n",
    "                negative_l = len(indexes_left) - positive_l\n",
    "                impurity_left = 1 - np.abs(positive_l - negative_l)/len(indexes_left)\n",
    "\n",
    "                #Create two children\n",
    "                n_left = TreeNode(key+1, depth+1)\n",
    "                n_right = TreeNode(key+2, depth+1)\n",
    "                n_left.depth = n.depth + 1\n",
    "                n_right.depth = n.depth + 1\n",
    "                n_left.data_idxs = indexes_left\n",
    "                n_right.data_idxs = indexes_right\n",
    "                key = key + 2\n",
    "\n",
    "                #Aggancio al padre\n",
    "                n.left_node = n_left\n",
    "                n.right_node= n_right\n",
    "                n.left_node_id = n_left.id\n",
    "                n.right_node_id = n_right.id\n",
    "\n",
    "\n",
    "                ones = np.count_nonzero(y[n_left.data_idxs])\n",
    "                nums = np.array([len(n_left.data_idxs) - ones, ones])\n",
    "                n_left.value = np.argmax(nums)\n",
    "                ones = np.count_nonzero(y[n_right.data_idxs])\n",
    "                nums = np.array([len(n_right.data_idxs) - ones, ones])\n",
    "                n_right.value = np.argmax(nums)\n",
    "\n",
    "\n",
    "                #Se è l'ultimo split vanno comunque messe foglie entrambe\n",
    "                if i == (n_splits - 1):\n",
    "                \n",
    "                    n_left.is_leaf = True\n",
    "                    n_right.is_leaf = True\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #Altrimenti controllo la purezza. Qui la foglia sarà il figlio sinistro\n",
    "                    if  impurity_left <= min_impurity:\n",
    "                        n_left.is_leaf = True        \n",
    "                        #stack.append(n_right)\n",
    "\n",
    "                    else:\n",
    "                        n_left.is_leaf = False\n",
    "                        \n",
    "                        stack.append(n_left)\n",
    "\n",
    "                    if impurity_right <= min_impurity:\n",
    "                        n_right.is_leaf = True\n",
    "                        #print(\"foglia a destra con \", len(n_right.data_idxs), \" punti. Punti a sinistra: \", len(n_left.data_idxs))\n",
    "                        #stack.append(n_left)\n",
    "                    else:\n",
    "                        n_right.is_leaf = False\n",
    "                        stack.append(n_right)\n",
    "                        \n",
    "                    n_right.impurity = impurity_right\n",
    "                    n_left.impurity = impurity_left\n",
    "                    \n",
    "\n",
    "                i = i + 1\n",
    "            else:\n",
    "                ones = np.count_nonzero(y[n.data_idxs])\n",
    "                nums = np.array([len(n.data_idxs) - ones, ones])\n",
    "                n.value = np.argmax(nums)\n",
    "                n.is_leaf = True\n",
    "                \n",
    "        else:\n",
    "            ones = np.count_nonzero(y[n.data_idxs])\n",
    "            nums = np.array([len(n.data_idxs) - ones, ones])\n",
    "            n.value = np.argmax(nums)\n",
    "            n.is_leaf = True\n",
    "            #print(\"none\")\n",
    "    \n",
    "    return node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46497f75c194891a06be384abf2d664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aldinucci/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left: 302\n",
      "right: 153\n",
      "left: 278\n",
      "right: 24\n",
      "left: 3\n",
      "right: 21\n",
      "left: 2\n",
      "right: 19\n",
      "left: 1\n",
      "right: 18\n",
      "left: 5\n",
      "right: 148\n",
      "left: 4\n",
      "right: 1\n",
      "\n",
      "CART: 0.9210526315789473 +- 0.0 Mean_depth_cart: 7.0 Sparse: 0.956140350877193 +- 0.0 Mean_depth_sparse: 5.0\n"
     ]
    }
   ],
   "source": [
    "sp, cart = [], []\n",
    "sp_depth, cart_depth = [], []\n",
    "for i in tqdm(range(1)):\n",
    "    sleep(3)\n",
    "    #Spambase\n",
    "    #data = pd.read_csv(\"datasets/spambase.csv\")\n",
    "    #X = data.to_numpy()\n",
    "    #y = X[:, -1].astype(int)\n",
    "    #X = X[:, 0:-1]\n",
    "    \n",
    "    #Banknote\n",
    "    #X = np.load(\"datasets/banknote_train.npy\")\n",
    "    #y = np.load(\"datasets/banknote_label.npy\")\n",
    "    \n",
    "    \n",
    "    #Cancer\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    \n",
    "    #X = X[y!=2]\n",
    "    #y = y[y!=2]\n",
    "    \n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = i)\n",
    "    \n",
    "    \n",
    "    #CART\n",
    "    clf = tree.DecisionTreeClassifier(random_state=i, min_samples_leaf = 1, min_impurity_split = 1e-2)\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    #CART TAO\n",
    "    cart_tree = ClassificationTree()\n",
    "    cart_tree.initialize_from_CART(X, y, clf)\n",
    "    tao = TAO(cart_tree)\n",
    "    tao.evolve(X, y)\n",
    "    preds = cart_tree.predict_data(X_test, cart_tree.tree[0])\n",
    "    cart_score = cart_tree.score(preds, y_test)\n",
    "    cart_depth.append(cart_tree.depth)\n",
    "    #tree.plot_tree(clf)  \n",
    "    #c_score = clf.score(X_test, y_test)\n",
    "    #print(c_score)\n",
    "    #print(\"CART test: \", c_score)\n",
    "    cart.append(cart_score)\n",
    "    \n",
    "    #Sparse\n",
    "    root = train_tree(X, y, n_splits = 100, min_points_leaf = 1)\n",
    "    sparse_tree = ClassificationTree()\n",
    "    sp_depth.append(sparse_tree.get_depth(root))\n",
    "    sparse_tree.initialize(X, y, root)\n",
    "    tao = TAO(sparse_tree)\n",
    "    tao.evolve(X, y)\n",
    "    #sparse_tree.print_tree_structure()\n",
    "    preds = sparse_tree.predict_data(X_test, root)\n",
    "    sparse_score = sparse_tree.score(preds, y_test)\n",
    "    #print(sparse_score)\n",
    "    #print(\"Sparse minimal test: \", sparse_score)\n",
    "    sp.append(sparse_score)\n",
    "    \n",
    "\n",
    "print(\"CART: %s +- %s Mean_depth_cart: %s Sparse: %s +- %s Mean_depth_sparse: %s\" % (np.mean(cart), np.std(cart), np.mean(cart_depth), np.mean(sp), np.std(sp), np.mean(sp_depth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
